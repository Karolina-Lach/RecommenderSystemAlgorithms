{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dcc2b2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6967a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import pandas as pd\n",
    "import sampling\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import pickle\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc60226c",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a299197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS_SMALL = \"../EDA_files/ratings_small.parquet\"\n",
    "RECIPES_SMALL = \"../EDA_files/recipes_small.parquet\"\n",
    "INDEX_TO_RECIPE_OBJ = \"../EDA_files/index_to_recipe.obj\"\n",
    "RECIPE_TO_INDEX_OBJ = \"../EDA_files/recipe_to_index.obj\"\n",
    "\n",
    "ING_CLEAN_NO_COMMON = '../cleaned_files/ingredients_clean_without_common_words.obj'\n",
    "KEYWORDS_CLEAN = '../cleaned_files/keywords_cleaned.obj'\n",
    "CATEGORIES_CLEAN = '../cleaned_files/categories_cleaned.obj'\n",
    "NAMES_CLEAN = '../cleaned_files/names_cleaned.obj'\n",
    "\n",
    "RECIPES_DATA = \"../dataset/recipes.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3ee04",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc940d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_small = pd.read_parquet(RECIPES_SMALL)\n",
    "ratings_small = pd.read_parquet(RATINGS_SMALL)\n",
    "\n",
    "with open(RECIPE_TO_INDEX_OBJ, \"rb\") as input_file:\n",
    "    recipe_to_index = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5bcc8",
   "metadata": {},
   "source": [
    "## Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_small[\"Timestamp\"] = ratings_small.DateSubmitted.map(lambda x: int(x.timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb36ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_min_20 = sampling.get_rating_with_min_number(ratings_small, 20, col_name='AuthorId')\n",
    "recipe_min_20 = sampling.get_rating_with_min_number(ratings_small, 20, col_name='RecipeId')\n",
    "\n",
    "ratings_min_20 = author_min_20.merge(recipe_min_20, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "650069ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_sample = ratings_min_20.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa2740",
   "metadata": {},
   "source": [
    "## Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b77a44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_sample = recipes_small[recipes_small.RecipeId.isin(list(ratings_min_20.RecipeId))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240df5c",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07183c27",
   "metadata": {},
   "source": [
    "## Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acdefd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 310022 entries, 0 to 310021\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype              \n",
      "---  ------         --------------   -----              \n",
      " 0   RecipeId       310022 non-null  int32              \n",
      " 1   AuthorId       310022 non-null  int32              \n",
      " 2   Rating         310022 non-null  int32              \n",
      " 3   Review         310022 non-null  object             \n",
      " 4   DateSubmitted  310022 non-null  datetime64[ns, UTC]\n",
      " 5   Timestamp      310022 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), int32(3), int64(1), object(1)\n",
      "memory usage: 13.0+ MB\n"
     ]
    }
   ],
   "source": [
    "ratings_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea7ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_sample.AuthorId = ratings_sample.AuthorId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "ratings_sample.RecipeId = ratings_sample.RecipeId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "\n",
    "ratings_dict = ratings_sample[['AuthorId', 'RecipeId', 'Timestamp']]\n",
    "ratings_dict = {name: np.array(value) for name, value in ratings_dict.items()}\n",
    "ratings = tf.data.Dataset.from_tensor_slices(ratings_dict)\n",
    "\n",
    "\n",
    "ratings = ratings.map(lambda x: {'AuthorId' : x['AuthorId'], \n",
    "                                 'RecipeId' : x['RecipeId'],\n",
    "                                 'Timestamp' : x['Timestamp']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d90ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AuthorId': b'2695', 'RecipeId': b'4807', 'Timestamp': 977924870}\n",
      "{'AuthorId': b'2312', 'RecipeId': b'810', 'Timestamp': 978452126}\n",
      "{'AuthorId': b'2695', 'RecipeId': b'12134', 'Timestamp': 979922414}\n",
      "{'AuthorId': b'5523', 'RecipeId': b'2713', 'Timestamp': 981052633}\n",
      "{'AuthorId': b'2312', 'RecipeId': b'8600', 'Timestamp': 984513096}\n",
      "{'AuthorId': b'6702', 'RecipeId': b'536', 'Timestamp': 986232934}\n",
      "{'AuthorId': b'2312', 'RecipeId': b'2886', 'Timestamp': 987584007}\n",
      "{'AuthorId': b'7802', 'RecipeId': b'8782', 'Timestamp': 990008786}\n",
      "{'AuthorId': b'10033', 'RecipeId': b'3748', 'Timestamp': 1027020496}\n",
      "{'AuthorId': b'2178', 'RecipeId': b'5478', 'Timestamp': 990740280}\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(10).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2b97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04362088",
   "metadata": {},
   "source": [
    "# Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65098048",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_sample.RecipeId = recipes_sample.RecipeId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "recipes_dict = recipes_sample[['RecipeId', 'Name']]\n",
    "recipes_dict = {name: np.array(value) for name, value in recipes_dict.items()}\n",
    "recipes = tf.data.Dataset.from_tensor_slices(recipes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05d71a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe_names = recipes.map(lambda x: x[\"Name\"])\n",
    "# recipes = recipes.map(lambda x: x[\"RecipeId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d08a1b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': b'Warm Chicken A La King', 'RecipeId': b'44'}\n",
      "{'Name': b'Buttermilk Pie', 'RecipeId': b'56'}\n",
      "{'Name': b'Black Bean, Corn, and Tomato Salad', 'RecipeId': b'62'}\n",
      "{'Name': b'Alfredo Sauce', 'RecipeId': b'76'}\n",
      "{'Name': b'Cheesy Scalloped Potato Side Dish', 'RecipeId': b'102'}\n",
      "{'Name': b'Blueberry Scones', 'RecipeId': b'116'}\n",
      "{'Name': b'Champagne Punch', 'RecipeId': b'129'}\n",
      "{'Name': b'Almond Fudge Banana Cake', 'RecipeId': b'142'}\n",
      "{'Name': b'Amish Friendship Bread and Starter', 'RecipeId': b'153'}\n",
      "{'Name': b'Light Cucumber Soup', 'RecipeId': b'155'}\n"
     ]
    }
   ],
   "source": [
    "for x in recipes.take(10).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e07b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipes_combined = tf.data.Dataset.from_tensor_slices(recipes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ac8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in recipes_combined.take(10).as_numpy_iterator():\n",
    "#     pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f288d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_merges = ratings_sample.merge(recipes_sample[[\"RecipeId\", \"Name\"]], on=\"RecipeId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05456f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_merged_dict = ratings_merges[['AuthorId', 'RecipeId', 'Timestamp', 'Name']]\n",
    "ratings_merged_dict = {name: np.array(value) for name, value in ratings_merged_dict.items()}\n",
    "ratings_merged = tf.data.Dataset.from_tensor_slices(ratings_merged_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f73b6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_merged = ratings_merged.map(lambda x: {'AuthorId' : x['AuthorId'], \n",
    "                                 'RecipeId' : x['RecipeId'],\n",
    "                                 'Timestamp' : x['Timestamp'],\n",
    "                                 'Name' : x['Name']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1df494",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15aaf0",
   "metadata": {},
   "source": [
    "## Creating dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ddc6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_ids_lookup = tf.keras.layers.StringLookup()\n",
    "recipe_ids_lookup.adapt(ratings.map(lambda x: x[\"RecipeId\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1758877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['[UNK]', '45809', '27208']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary: {recipe_ids_lookup.get_vocabulary()[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d253d",
   "metadata": {},
   "source": [
    "## Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783dd512",
   "metadata": {},
   "source": [
    "### Recipe id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dad22772",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_id_embedding = tf.keras.layers.Embedding(\n",
    "                        input_dim=recipe_ids_lookup.vocabulary_size(),\n",
    "                        output_dim=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76403beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_id_model = tf.keras.Sequential([recipe_ids_lookup, recipe_id_embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25076f36",
   "metadata": {},
   "source": [
    "### User id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be98aaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    }
   ],
   "source": [
    "user_id_lookup = tf.keras.layers.StringLookup()\n",
    "user_id_lookup.adapt(ratings.map(lambda x: x[\"AuthorId\"]))\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(user_id_lookup.vocab_size(), 32)\n",
    "user_id_model = tf.keras.Sequential([user_id_lookup, user_id_embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f505fdd2",
   "metadata": {},
   "source": [
    "## Normalizing timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89dee616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 977924870\n",
      "Timestamp: 978452126\n",
      "Timestamp: 979922414\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "    print(f\"Timestamp: {x['Timestamp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e2aab40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized timestamp: [-2.5186186]\n",
      "Normalized timestamp: [-2.5129895]\n",
      "Normalized timestamp: [-2.4972913]\n"
     ]
    }
   ],
   "source": [
    "timestamp_normalization = tf.keras.layers.Normalization(axis=None)\n",
    "\n",
    "timestamp_normalization.adapt(ratings.map(lambda x: x['Timestamp']).batch(1024))\n",
    "\n",
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "    print(f\"Normalized timestamp: {timestamp_normalization(x['Timestamp'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c59501",
   "metadata": {},
   "source": [
    "## Discretization timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e608029",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"Timestamp\"]).batch(100)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "274191b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets: [9.77924870e+08 9.78556604e+08 9.79188338e+08]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Buckets: {timestamp_buckets[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9396cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp embedding: [[-0.01477839 -0.02660116 -0.04763311  0.00071366 -0.02035424  0.0490985\n",
      "  -0.04404924 -0.03259118 -0.00255934 -0.04514308  0.03547132 -0.01090539\n",
      "   0.04388595 -0.00482281 -0.01667156  0.03670093 -0.01225809 -0.00892891\n",
      "  -0.00916835 -0.03770336  0.00340996  0.01761632 -0.04229828  0.04998362\n",
      "  -0.00917307 -0.03694183  0.0129084   0.02816662  0.01990917  0.01294592\n",
      "   0.00850926 -0.0246276 ]]\n"
     ]
    }
   ],
   "source": [
    "timestamp_embedding_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "    tf.keras.layers.Embedding(len(timestamp_buckets)+1, 32)\n",
    "])\n",
    "\n",
    "for timestamp in ratings.take(1).map(lambda x: x[\"Timestamp\"]).batch(1).as_numpy_iterator():\n",
    "    print(f\"Timestamp embedding: {timestamp_embedding_model(timestamp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8ddf5",
   "metadata": {},
   "source": [
    "## Processing text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61120d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = tf.keras.layers.TextVectorization()\n",
    "title_text.adapt(recipes.map(lambda x: x['Name']).batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34606142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[705   2  60 433 831]], shape=(1, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for row in recipes.take(1).map(lambda x: x['Name']).batch(1).as_numpy_iterator():\n",
    "    print(title_text(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a958e5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'warm chicken a la king'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_text.get_vocabulary()[705] + \" \" + title_text.get_vocabulary()[2] + \" \" + title_text.get_vocabulary()[60] + \" \" + title_text.get_vocabulary()[433] + \" \" + title_text.get_vocabulary()[831]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cec274",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be471244",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(lambda x: x[\"AuthorId\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e258caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_recipe_names = np.unique(np.concatenate(list(recipes.batch(1_000).map(lambda x: x[\"Name\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3990c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_recipe_ids = np.unique(np.concatenate(list(recipes.batch(1_000).map(lambda x: x[\"RecipeId\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f4112",
   "metadata": {},
   "source": [
    "## User model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6a42670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tfrs.models.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32)\n",
    "        ])\n",
    "        \n",
    "        self.timestamp_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(timestamp_buckets)+1, 32),\n",
    "        ])\n",
    "        \n",
    "        self.normalized_timestamp = tf.keras.layers.Normalization(axis=None)\n",
    "        self.normalized_timestamp.adapt(timestamps)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"AuthorId\"]),\n",
    "            self.timestamp_embedding(inputs[\"Timestamp\"]),\n",
    "            tf.reshape(self.normalized_timestamp(inputs[\"Timestamp\"]), (-1,1)),\n",
    "        ], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "445fb34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = UserModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97cfc0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation: [ 0.04997486 -0.02538269 -0.0289157 ]\n"
     ]
    }
   ],
   "source": [
    "for row in ratings.batch(1).take(1):\n",
    "    print(f\"Representation: {user_model(row)[0, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cd334",
   "metadata": {},
   "source": [
    "## Recipe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e2c138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeModel(tfrs.models.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        max_tokens = 10_000\n",
    "        \n",
    "        self.recipe_id_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=unique_recipe_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_recipe_ids)+1, 32)\n",
    "        ])\n",
    "        \n",
    "        self.name_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_tokens)\n",
    "        \n",
    "        self.name_text_embedding = tf.keras.Sequential([\n",
    "            self.name_vectorizer,\n",
    "            tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "            tf.keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        \n",
    "        self.name_vectorizer.adapt(recipes.map(lambda x: x['Name']))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "#         print(inputs)\n",
    "        return tf.concat([\n",
    "            self.recipe_id_embedding(inputs[\"RecipeId\"])\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe865d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_model = RecipeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38be9837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'bytes'> input: b'44'\n",
      "Consider rewriting this model with the Functional API.\n",
      "tf.Tensor(\n",
      "[ 0.04185012  0.03064827  0.00648462  0.0319382   0.0093194  -0.00619936\n",
      "  0.01752664  0.0411532   0.04666496 -0.04103626  0.01227326 -0.03131478\n",
      "  0.01060254  0.03605009 -0.04726265  0.04982701  0.04543876  0.04340408\n",
      "  0.00903907  0.04870263  0.04487706  0.04395784 -0.04400523 -0.00641304\n",
      " -0.04760621 -0.02025449 -0.02897121 -0.04928046 -0.03056639  0.0095516\n",
      " -0.01649749  0.03808099], shape=(32,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x in recipes.take(1).as_numpy_iterator():\n",
    "#     print(x)\n",
    "    print(recipe_model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d04b7",
   "metadata": {},
   "source": [
    "## Query model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cdac7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModel(tfrs.models.Model):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.embedding_model = UserModel()\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "        \n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "            \n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        feature_embedding = self.embedding_model(inputs)\n",
    "        return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff854c32",
   "metadata": {},
   "source": [
    "## Candidate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "67d66c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateModel(tfrs.models.Model):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.embedding_model = RecipeModel()\n",
    "        \n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "        \n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "            \n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        feature_embedding = self.embedding_model(inputs)\n",
    "        return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceccccee",
   "metadata": {},
   "source": [
    "## Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6825c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(tfrs.models.Model):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.query_model = QueryModel(layer_sizes)\n",
    "        self.candidate_model = CandidateModel(layer_sizes)\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=recipes.batch(128).map(self.candidate_model),\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    def compute_loss(self, features, training=False):\n",
    "        print(features)\n",
    "        query_embeddings = self.query_model({\n",
    "            \"AuthorId\": features[\"AuthorId\"],\n",
    "            \"Timestamp\": features[\"Timestamp\"],\n",
    "        })\n",
    "        \n",
    "        recipe_embeddings = self.candidate_model({\n",
    "            \"RecipeId\": features[\"RecipeId\"],\n",
    "            \"Name\": features[\"Name\"]\n",
    "        })\n",
    "        \n",
    "        return self.task(\n",
    "            query_embeddings, recipe_embeddings, compute_metrics=not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7806dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = ratings_min_20.shape[0]\n",
    "train_size = int(0.8 * size)\n",
    "test_size = size - train_size\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings_merged.shuffle(size, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(train_size)\n",
    "test = shuffled.take(train_size).take(test_size)\n",
    "\n",
    "cached_train = train.shuffle(1_000_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c7aac04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AuthorId': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=string>, 'RecipeId': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'Timestamp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=int64>, 'Name': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=string>}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 75, in train_step\n        gradients = tape.gradient(total_loss, self.trainable_variables)\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 2308, in trainable_variables\n        return self.trainable_weights\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2104, in trainable_weights\n        trainable_variables += trackable_obj.trainable_variables\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 2308, in trainable_variables\n        return self.trainable_weights\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2104, in trainable_weights\n        trainable_variables += trackable_obj.trainable_variables\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 2308, in trainable_variables\n        return self.trainable_weights\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2104, in trainable_weights\n        trainable_variables += trackable_obj.trainable_variables\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 2308, in trainable_variables\n        return self.trainable_weights\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2099, in trainable_weights\n        self._assert_weights_created()\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\sequential.py\", line 471, in _assert_weights_created\n        super(functional.Functional, self)._assert_weights_created()  # pylint: disable=bad-super-call\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2736, in _assert_weights_created\n        raise ValueError(f'Weights for model {self.name} have not yet been '\n\n    ValueError: Weights for model sequential_52 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\UYTKOW~1\\AppData\\Local\\Temp/ipykernel_6292/2023865555.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m one_layer_history = model.fit(\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mcached_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcached_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 75, in train_step\n        gradients = tape.gradient(total_loss, self.trainable_variables)\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 2308, in trainable_variables\n        return self.trainable_weights\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2104, in trainable_weights\n        trainable_variables += trackable_obj.trainable_variables\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 2308, in trainable_variables\n        return self.trainable_weights\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2104, in trainable_weights\n        trainable_variables += trackable_obj.trainable_variables\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 2308, in trainable_variables\n        return self.trainable_weights\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2104, in trainable_weights\n        trainable_variables += trackable_obj.trainable_variables\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 2308, in trainable_variables\n        return self.trainable_weights\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2099, in trainable_weights\n        self._assert_weights_created()\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\sequential.py\", line 471, in _assert_weights_created\n        super(functional.Functional, self)._assert_weights_created()  # pylint: disable=bad-super-call\n    File \"C:\\anaconda\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\training.py\", line 2736, in _assert_weights_created\n        raise ValueError(f'Weights for model {self.name} have not yet been '\n\n    ValueError: Weights for model sequential_52 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "model = CombinedModel([32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "accuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaaeb43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
