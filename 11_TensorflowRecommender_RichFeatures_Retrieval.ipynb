{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60389781",
   "metadata": {
    "id": "60389781"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade83d80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ade83d80",
    "outputId": "02a39a28-8c44-4a97-b095-563898ddef2f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import pandas as pd\n",
    "import sampling\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import pickle\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Flatten   \n",
    "from tensorflow.keras.layers import Dense     \n",
    "\n",
    "import TensorflowRichFeatures as tfrs_rich\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa97de",
   "metadata": {
    "id": "56fa97de"
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62449557",
   "metadata": {
    "id": "62449557"
   },
   "source": [
    "## Load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3614e9",
   "metadata": {
    "id": "cf3614e9"
   },
   "outputs": [],
   "source": [
    "RATINGS_BASE = \"./drive/MyDrive/Colab Notebooks/Recipes_new/Data/base/ratings_base.parquet\"\n",
    "RECIPES_BASE = \"./drive/MyDrive/Colab Notebooks/Recipes_new/Data/base/recipes_base.parquet\"\n",
    "\n",
    "CONCAT_ING_CAT= \"./drive/MyDrive/Colab Notebooks/Recipes_new/Data/samples/concatenated_ing_cat_df.obj\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cfafb",
   "metadata": {
    "id": "f29cfafb"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98504e",
   "metadata": {
    "id": "fa98504e"
   },
   "outputs": [],
   "source": [
    "recipes_small = pd.read_parquet(RECIPES_BASE)\n",
    "ratings_small = pd.read_parquet(RATINGS_BASE)\n",
    "\n",
    "with open(CONCAT_ING_CAT, \"rb\") as input_file:\n",
    "    concatenated_ing_cat_df = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dc5a3",
   "metadata": {
    "id": "b54dc5a3"
   },
   "source": [
    "## Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02782af",
   "metadata": {
    "id": "a02782af"
   },
   "outputs": [],
   "source": [
    "ratings_sample = sampling.get_ratings_with_min_number_list(ratings_small, [20, 10])\n",
    "ratings_sample.RecipeId = ratings_sample.RecipeId.apply(lambda x: int(x))\n",
    "recipes_small.RecipeId = recipes_small.RecipeId.apply(lambda x: int(x))\n",
    "recipe_ids_in_sample = list(set(ratings_sample.RecipeId))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b70ec",
   "metadata": {
    "id": "b52b70ec"
   },
   "source": [
    "## Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab7d90",
   "metadata": {
    "id": "80ab7d90"
   },
   "outputs": [],
   "source": [
    "recipes_subset = recipes_small[[\"RecipeId\"]].merge(concatenated_ing_cat_df, on=\"RecipeId\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba39af",
   "metadata": {
    "id": "aaba39af"
   },
   "outputs": [],
   "source": [
    "recipes_subset[\"Ingredients_Category\"] = recipes_subset[\"Concatenated\"].map(lambda x: \" \".join(x))\n",
    "recipes_subset = recipes_subset[recipes_subset.RecipeId.isin(recipe_ids_in_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed74d7",
   "metadata": {
    "id": "e2ed74d7"
   },
   "outputs": [],
   "source": [
    "merged_dataset = ratings_sample.merge(recipes_subset, on=\"RecipeId\", how=\"inner\")\n",
    "merged_dataset.drop(columns=[\"Concatenated\", \"DateSubmitted\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed0f2e",
   "metadata": {
    "id": "96ed0f2e"
   },
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "znJQtPb_IHDa",
   "metadata": {
    "id": "znJQtPb_IHDa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as train_test_split\n",
    "\n",
    "def create_train_test_dataframe(ratings_df, test_size, random_state):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(ratings_df[[\"AuthorId\", \"RecipeId\", \"Ingredients_Category\"]], \n",
    "                                                        ratings_df[[\"Rating\"]], \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state, \n",
    "                                                        stratify=ratings_df[\"AuthorId\"])\n",
    "    trainset = x_train.merge(y_train, left_index=True, right_index=True)\n",
    "    testset = x_test.merge(y_test, left_index=True, right_index=True)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LiM8T_qhJd2Y",
   "metadata": {
    "id": "LiM8T_qhJd2Y"
   },
   "outputs": [],
   "source": [
    "trainset, testset = create_train_test_dataframe(merged_dataset, 0.2, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8380e44",
   "metadata": {
    "id": "d8380e44"
   },
   "source": [
    "## Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f721c5",
   "metadata": {
    "id": "52f721c5"
   },
   "outputs": [],
   "source": [
    "recipes_subset.RecipeId = recipes_subset.RecipeId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "\n",
    "recipes_dict = recipes_subset[['RecipeId','Ingredients_Category']]\n",
    "recipes_dict = {name: np.array(value) for name, value in recipes_dict.items()}\n",
    "recipes = tf.data.Dataset.from_tensor_slices(recipes_dict)\n",
    "\n",
    "\n",
    "recipes = recipes.map(lambda x: {'RecipeId' : x['RecipeId'],\n",
    "                                 'Ingredients_Category' : x['Ingredients_Category']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef613e45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef613e45",
    "outputId": "9701d9d9-bf7d-4f49-c90d-e8f16bd4880a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ingredients_Category': b'black pepper butter button mushroom celery chicken f'\n",
      "                         b'lour milk parsley pepper pimiento worcestershire sau'\n",
      "                         b'ce chicken',\n",
      " 'RecipeId': b'44'}\n"
     ]
    }
   ],
   "source": [
    "for x in recipes.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I3jNDHHZ9aeH",
   "metadata": {
    "id": "I3jNDHHZ9aeH"
   },
   "outputs": [],
   "source": [
    "recipes_dict = recipes_subset[['RecipeId']]\n",
    "recipes_dict = {name: np.array(value) for name, value in recipes_dict.items()}\n",
    "recipes_dataset = tf.data.Dataset.from_tensor_slices(recipes_dict)\n",
    "\n",
    "\n",
    "recipes_dataset = recipes_dataset.map(lambda x: {'RecipeId' : x['RecipeId']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AY_WNnop9ke3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AY_WNnop9ke3",
    "outputId": "be191358-dc50-404b-81e3-969ca2ca5f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RecipeId': b'44'}\n"
     ]
    }
   ],
   "source": [
    "for x in recipes_dataset.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332ae27",
   "metadata": {
    "id": "2332ae27"
   },
   "source": [
    "## Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b6c3d",
   "metadata": {
    "id": "409b6c3d"
   },
   "outputs": [],
   "source": [
    "ratings_sample.AuthorId = ratings_sample.AuthorId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "ratings_sample.RecipeId = ratings_sample.RecipeId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "\n",
    "ratings_dict = ratings_sample[['AuthorId', 'RecipeId']]\n",
    "ratings_dict = {name: np.array(value) for name, value in ratings_dict.items()}\n",
    "ratings = tf.data.Dataset.from_tensor_slices(ratings_dict)\n",
    "\n",
    "\n",
    "ratings = ratings.map(lambda x: {'AuthorId' : x['AuthorId'], \n",
    "                                 'RecipeId' : x['RecipeId']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9751b5a",
   "metadata": {
    "id": "d9751b5a"
   },
   "source": [
    "## Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6cac14",
   "metadata": {
    "id": "0f6cac14"
   },
   "outputs": [],
   "source": [
    "merged_dataset.AuthorId = merged_dataset.AuthorId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "merged_dataset.RecipeId = merged_dataset.RecipeId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "\n",
    "merged_dict = merged_dataset[['AuthorId', 'RecipeId', 'Ingredients_Category']]\n",
    "merged_dict = {name: np.array(value) for name, value in merged_dict.items()}\n",
    "merged_dataset = tf.data.Dataset.from_tensor_slices(merged_dict)\n",
    "\n",
    "\n",
    "merged_dataset = merged_dataset.map(lambda x: {'AuthorId' : x['AuthorId'], \n",
    "                                 'RecipeId' : x['RecipeId'],\n",
    "                                 'Ingredients_Category' : x['Ingredients_Category']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46bb939",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a46bb939",
    "outputId": "0756f016-ef01-4f56-a45b-3aee0dd5e454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AuthorId': b'2312',\n",
      " 'Ingredients_Category': b'cayenne pepper chicken breast cumin garlic ginger le'\n",
      "                         b'mon lemon juice nutmeg paprika turmeric water chicke'\n",
      "                         b'n breast',\n",
      " 'RecipeId': b'780'}\n"
     ]
    }
   ],
   "source": [
    "for x in merged_dataset.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0CVoX5lKcPG",
   "metadata": {
    "id": "b0CVoX5lKcPG"
   },
   "outputs": [],
   "source": [
    "trainset.AuthorId = trainset.AuthorId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "trainset.RecipeId = trainset.RecipeId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "\n",
    "trainset_dict = trainset[['AuthorId', 'RecipeId', 'Ingredients_Category']]\n",
    "trainset_dict = {name: np.array(value) for name, value in trainset.items()}\n",
    "trainset_dataset = tf.data.Dataset.from_tensor_slices(trainset_dict)\n",
    "\n",
    "\n",
    "trainset_dataset = trainset_dataset.map(lambda x: {'AuthorId' : x['AuthorId'], \n",
    "                                 'RecipeId' : x['RecipeId'],\n",
    "                                 'Ingredients_Category' : x['Ingredients_Category']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uovbmL9jKsJ4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uovbmL9jKsJ4",
    "outputId": "3c5d709a-c39c-49c0-cca6-8ea1d495edcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AuthorId': b'68727',\n",
      " 'Ingredients_Category': b'carrot celery dry marjoram ham hock onion pepper ',\n",
      " 'RecipeId': b'112831'}\n"
     ]
    }
   ],
   "source": [
    "for x in trainset_dataset.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lOToRbCHKvTU",
   "metadata": {
    "id": "lOToRbCHKvTU"
   },
   "outputs": [],
   "source": [
    "testset.AuthorId = testset.AuthorId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "testset.RecipeId = testset.RecipeId.map(lambda x: bytes(str(x), 'utf-8'))\n",
    "\n",
    "testset_dict = testset[['AuthorId', 'RecipeId', 'Ingredients_Category']]\n",
    "testset_dict = {name: np.array(value) for name, value in testset.items()}\n",
    "testset_dataset = tf.data.Dataset.from_tensor_slices(testset_dict)\n",
    "\n",
    "\n",
    "testset_dataset = testset_dataset.map(lambda x: {'AuthorId' : x['AuthorId'], \n",
    "                                 'RecipeId' : x['RecipeId'],\n",
    "                                 'Ingredients_Category' : x['Ingredients_Category']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f35af9",
   "metadata": {
    "id": "a4f35af9"
   },
   "source": [
    "## Unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c7614",
   "metadata": {
    "id": "786c7614"
   },
   "outputs": [],
   "source": [
    "recipe_ids = recipes.batch(1).map(lambda x: x[\"RecipeId\"])\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"AuthorId\"])\n",
    "\n",
    "unique_recipe_ids = np.unique(np.concatenate(list(recipe_ids)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb17a4",
   "metadata": {
    "id": "31eb17a4"
   },
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d675669",
   "metadata": {
    "id": "8d675669"
   },
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57134118",
   "metadata": {
    "id": "57134118"
   },
   "outputs": [],
   "source": [
    "size = ratings_sample.shape[0]\n",
    "train_size = int(trainset.shape[0])\n",
    "val_size = int(0.15 * size)\n",
    "test_size = int(testset.shape[0])\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "test_shuffled = testset_dataset.shuffle(test_size, seed=42, reshuffle_each_iteration=False)\n",
    "train_shuffled = trainset_dataset.shuffle(train_size, seed=42, reshuffle_each_iteration=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bOqyoazoLx1D",
   "metadata": {
    "id": "bOqyoazoLx1D"
   },
   "outputs": [],
   "source": [
    "val = train_shuffled.take(val_size)\n",
    "\n",
    "cached_train = train_shuffled.batch(8192).cache()\n",
    "cached_test = test_shuffled.batch(4096).cache()\n",
    "cached_val = val.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4836d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d4836d7",
    "outputId": "94c96052-d809-4d27-ee53-bcf8131a44a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec={'AuthorId': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'RecipeId': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'Ingredients_Category': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_train.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9562712",
   "metadata": {
    "id": "d9562712"
   },
   "source": [
    "## One layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e50d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "863e50d9",
    "outputId": "9c67a5ec-3a52-482d-9780-05d14be1d0d6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init combined model\n",
      "Query model init\n",
      "USER MODEL INIT\n",
      "Candidate model init\n",
      "RECIPE MODEL INIT\n",
      "Candidate model call\n",
      "Inputs:  {'RecipeId': <tf.Tensor 'args_1:0' shape=(None,) dtype=string>, 'Ingredients_Category': <tf.Tensor 'args_0:0' shape=(None,) dtype=string>}\n",
      "Recipe model call\n",
      "INPUTS:  {'RecipeId': <tf.Tensor 'args_1:0' shape=(None,) dtype=string>, 'Ingredients_Category': <tf.Tensor 'args_0:0' shape=(None,) dtype=string>}\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "model = tfrs_rich.CombinedModel(layer_sizes=[32], \n",
    "                      unique_recipe_ids=unique_recipe_ids, \n",
    "                      unique_user_ids=unique_user_ids, \n",
    "                      recipes_dataset=recipes,\n",
    "                                verbose=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc9205",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67fc9205",
    "outputId": "e42e082b-d240-4842-ed41-9baba6ff023e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f583d6e63d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_val,\n",
    "    validation_freq=5,\n",
    "    epochs=5,\n",
    "    verbose=1)\n",
    "\n",
    "# model.load_weights(\"./drive/MyDrive/Colab Notebooks/Recipes_new/Data/TFRS/features/retrieval/20_10/model_1/model_1_500_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZkTDiMvyxEUf",
   "metadata": {
    "id": "ZkTDiMvyxEUf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kopia 11_TensorflowRecommender_RichFeatures_Retrieval_ING_CAT_NO_TIME.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
